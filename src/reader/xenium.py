import json
import os
import re
import tempfile
import zipfile
from collections.abc import Mapping
from pathlib import Path
from types import MappingProxyType
from typing import Any
import warnings
import logging

import dask.array as da
import numpy as np
import packaging.version
import pandas as pd
import pyarrow.parquet as pq
import tifffile
import zarr
from anndata import AnnData
from dask.dataframe import read_parquet
from dask_image.imread import imread
from geopandas import GeoDataFrame
from joblib import Parallel, delayed
from pyarrow import Table
from shapely import Polygon
from spatialdata import SpatialData
from spatialdata._core.query.relational_query import get_element_instances
from spatialdata._types import ArrayLike
from spatialdata.models import Image2DModel, Labels2DModel, PointsModel, ShapesModel, TableModel
from spatialdata.transformations.transformations import Affine, Identity, Scale
from xarray import DataArray, DataTree

from spatialdata_io._constants._constants import XeniumKeys
from spatialdata_io._docs import inject_docs
from spatialdata_io._utils import deprecation_alias
from spatialdata_io.readers._utils._read_10x_h5 import _read_10x_h5
from spatialdata_io.readers._utils._utils import _initialize_raster_models_kwargs

@deprecation_alias(cells_as_shapes="cells_as_circles", cell_boundaries="cells_boundaries", cell_labels="cells_labels")
@inject_docs(xx=XeniumKeys)
def xenium(
    path: str | Path,
    *,
    cells_boundaries: bool = True,
    nucleus_boundaries: bool = True,
    cells_as_circles: bool | None = None,
    cells_labels: bool = True,
    nucleus_labels: bool = True,
    transcripts: bool = True,
    morphology_mip: bool = True,
    morphology_focus: bool = True,
    aligned_images: bool = True,
    cells_table: bool = True,
    n_jobs: int = 1,
    imread_kwargs: Mapping[str, Any] = MappingProxyType({}),
    image_models_kwargs: Mapping[str, Any] = MappingProxyType({}),
    labels_models_kwargs: Mapping[str, Any] = MappingProxyType({})) -> SpatialData:
    """Load a 10X Genomics Xenium dataset into a SpatialData object.

    This function reads spatial transcriptomics data generated by Xenium, including geometry,
    labels, transcripts, morphology images, and aligned images. It handles backward compatibility,
    default warnings, and performance tuning through multiple input options.

    Args:
        path (str or Path): Path to the Xenium dataset directory.
        cells_boundaries (bool, optional): Whether to load cell boundary polygons. Defaults to True.
        nucleus_boundaries (bool, optional): Whether to load nucleus boundary polygons. Defaults to True.
        cells_as_circles (bool or None, optional): Whether to load cells as circles. If None (default), 
            a deprecation warning is raised and treated as True.
        cells_labels (bool, optional): Whether to load rasterized cell labels. Defaults to True.
        nucleus_labels (bool, optional): Whether to load rasterized nucleus labels. Defaults to True.
        transcripts (bool, optional): Whether to load transcript coordinates. Defaults to True.
        morphology_mip (bool, optional): Whether to load MIP morphology image (pre-v2.0.0 only). Defaults to True.
        morphology_focus (bool, optional): Whether to load morphology focus image. Defaults to True.
        aligned_images (bool, optional): Whether to load additional aligned H&E or IF images. Defaults to True.
        cells_table (bool, optional): Whether to load the cell annotations table. Defaults to True.
        n_jobs (int, optional): Number of parallel processes to use. Defaults to 1.
        imread_kwargs (Mapping[str, Any], optional): Extra arguments for image reading. Defaults to empty MappingProxyType.
        image_models_kwargs (Mapping[str, Any], optional): Extra arguments for image models. Defaults to empty MappingProxyType.
        labels_models_kwargs (Mapping[str, Any], optional): Extra arguments for label models. Defaults to empty MappingProxyType.

    Returns:
        SpatialData: A `SpatialData` object containing images, labels, tables, points, and polygon shapes.

    Raises:
        ValueError: If required files are missing or incorrectly formatted, particularly for morphology images.

    Notes:
        - The default for `cells_as_circles` is currently True but will change to False in a future release.
        - For newer Xenium versions (>= 2.0.0), morphology data is structured differently.
        - Cells must be linked to metadata tables for shape/polygon information to be valid.
        - Mismatched `cell_id` values across sources will emit warnings.

    Example:
        >>> from spatialdata_io import xenium
        >>> sdata = xenium("path/to/xenium/data", cells_as_circles=True)
        >>> sdata["table"].obs["region"] = "cell_labels"
        >>> sdata.set_table_annotates_spatialelement(
        ...     table_name="table",
        ...     region="cell_labels",
        ...     region_key="region",
        ...     instance_key="cell_labels"
        ... )
        >>> sdata.write("path/to/output.zarr")
    """
    
    if cells_as_circles is None:
        cells_as_circles = True
        warnings.warn(
            "The default value of `cells_as_circles` will change to `False` in the next release. "
            "Please pass `True` explicitly to maintain the current behavior.",
            DeprecationWarning,
            stacklevel=3)
    image_models_kwargs, labels_models_kwargs = _initialize_raster_models_kwargs(
        image_models_kwargs, labels_models_kwargs)
    path = Path(path)
    
    with open(path / XeniumKeys.XENIUM_SPECS) as f:
        specs = json.load(f)
    # to trigger the warning if the version cannot be parsed
    version = _parse_version_of_xenium_analyzer(specs, hide_warning=False)

    specs["region"] = "cell_circles" if cells_as_circles else "cell_labels"

    # the table is required in some cases
    if not cells_table:
        
        if cells_as_circles:
            logging.info(
                'When "cells_as_circles" is set to `True` reading the table is required; setting `cell_annotations` to '
                "`True`.")
            cells_table = True
            
        if cells_boundaries or nucleus_boundaries:
            logging.info(
                'When "cell_boundaries" or "nucleus_boundaries" is set to `True` reading the table is required; '
                "setting `cell_annotations` to `True`.")
            cells_table = True

    if cells_table:
        return_values = _get_tables_and_circles(path, cells_as_circles, specs)
        
        if cells_as_circles:
            table, circles = return_values
        else:
            table = return_values
    else:
        table = None

    if version is not None and version >= packaging.version.parse("2.0.0") and table is not None:
        cell_summary_table = _get_cells_metadata_table_from_zarr(path, XeniumKeys.CELLS_ZARR, specs)
        
        if not cell_summary_table[XeniumKeys.CELL_ID].equals(table.obs[XeniumKeys.CELL_ID]):
            warnings.warn(
                'The "cell_id" column in the cells metadata table does not match the "cell_id" column in the annotation'
                " table. This could be due to trying to read a new version that is not supported yet. Please "
                "report this issue.",
                UserWarning,
                stacklevel=2)
        table.obs[XeniumKeys.Z_LEVEL] = cell_summary_table[XeniumKeys.Z_LEVEL]
        table.obs[XeniumKeys.NUCLEUS_COUNT] = cell_summary_table[XeniumKeys.NUCLEUS_COUNT]

    polygons = {}
    labels = {}
    tables = {}
    points = {}
    images = {}

    # From the public release notes here:
    # https://www.10xgenomics.com/support/software/xenium-onboard-analysis/latest/release-notes/release-notes-for-xoa
    # we see that for distinguishing between the nuclei of polinucleated cells, the `label_id` column is used.
    # This column is currently not found in the preview data, while I think it is needed in order to unambiguously match
    # nuclei to cells. Therefore for the moment we only link the table to the cell labels, and not to the nucleus
    # labels.
    if nucleus_labels:
        labels["nucleus_labels"], _ = _get_labels_and_indices_mapping(
            path,
            XeniumKeys.CELLS_ZARR,
            specs,
            mask_index=0,
            labels_name="nucleus_labels",
            labels_models_kwargs=labels_models_kwargs)
        
    if cells_labels:
        labels["cell_labels"], cell_labels_indices_mapping = _get_labels_and_indices_mapping(
            path,
            XeniumKeys.CELLS_ZARR,
            specs,
            mask_index=1,
            labels_name="cell_labels",
            labels_models_kwargs=labels_models_kwargs)
        
        if cell_labels_indices_mapping is not None and table is not None:
            
            if not pd.DataFrame.equals(cell_labels_indices_mapping["cell_id"], table.obs[str(XeniumKeys.CELL_ID)]):
                warnings.warn(
                    "The cell_id column in the cell_labels_table does not match the cell_id column derived from the "
                    "cell labels data. This could be due to trying to read a new version that is not supported yet. "
                    "Please report this issue.",
                    UserWarning,
                    stacklevel=2)
            else:
                table.obs["cell_labels"] = cell_labels_indices_mapping["label_index"]
                
                if not cells_as_circles:
                    table.uns[TableModel.ATTRS_KEY][TableModel.INSTANCE_KEY] = "cell_labels"

    if nucleus_boundaries:
        polygons["nucleus_boundaries"] = _get_polygons(
            path,
            XeniumKeys.NUCLEUS_BOUNDARIES_FILE,
            specs,
            n_jobs,
            idx=table.obs[str(XeniumKeys.CELL_ID)].copy())

    if cells_boundaries:
        polygons["cell_boundaries"] = _get_polygons(
            path,
            XeniumKeys.CELL_BOUNDARIES_FILE,
            specs,
            n_jobs,
            idx=table.obs[str(XeniumKeys.CELL_ID)].copy())

    if transcripts:
        points["transcripts"] = _get_points(path, specs)

    if version is None or version < packaging.version.parse("2.0.0"):
        
        if morphology_mip:
            images["morphology_mip"] = _get_images(
                path,
                XeniumKeys.MORPHOLOGY_MIP_FILE,
                imread_kwargs,
                image_models_kwargs)
            
        if morphology_focus:
            images["morphology_focus"] = _get_images(
                path,
                XeniumKeys.MORPHOLOGY_FOCUS_FILE,
                imread_kwargs,
                image_models_kwargs)
    else:
        
        if morphology_focus:
            morphology_focus_dir = path / XeniumKeys.MORPHOLOGY_FOCUS_DIR
            files = {f for f in os.listdir(morphology_focus_dir) if f.endswith(".ome.tif") and not f.startswith("._")}
            
            if len(files) not in [1, 4]:
                raise ValueError(
                    "Expected 1 (no segmentation kit) or 4 (segmentation kit) files in the morphology focus directory, "
                    f"found {len(files)}: {files}")
                
            if files != {XeniumKeys.MORPHOLOGY_FOCUS_CHANNEL_IMAGE.value.format(i) for i in range(len(files))}:
                raise ValueError(
                    "Expected files in the morphology focus directory to be named as "
                    f"{XeniumKeys.MORPHOLOGY_FOCUS_CHANNEL_IMAGE.value.format(0)} to "
                    f"{XeniumKeys.MORPHOLOGY_FOCUS_CHANNEL_IMAGE.value.format(len(files) - 1)}, found {files}")
                
            if len(files) == 1:
                channel_names = {
                    0: XeniumKeys.MORPHOLOGY_FOCUS_CHANNEL_0.value}
            else:
                channel_names = {
                    0: XeniumKeys.MORPHOLOGY_FOCUS_CHANNEL_0.value,
                    1: XeniumKeys.MORPHOLOGY_FOCUS_CHANNEL_1.value,
                    2: XeniumKeys.MORPHOLOGY_FOCUS_CHANNEL_2.value,
                    3: XeniumKeys.MORPHOLOGY_FOCUS_CHANNEL_3.value}
            # this reads the scale 0 for all the 1 or 4 channels (the other files are parsed automatically)
            # dask.image.imread will call tifffile.imread which will give a warning saying that reading multi-file
            # pyramids is not supported; since we are reading the full scale image and reconstructing the pyramid, we
            # can ignore this

            class IgnoreSpecificMessage(logging.Filter):
                """A logging filter that suppresses specific tifffile warnings.
            
                This filter is designed to ignore the known warning from `tifffile` regarding
                unsupported multi-file pyramids in OME-TIFF files. It allows all other log messages
                to pass through.
            
                Methods:
                    filter(record): Determines whether a given log record should be displayed.
                """
            
                def filter(self, record: logging.LogRecord) -> bool:
                    """Determine if the log record should be logged.
            
                    Suppresses log records that contain the specific warning message about
                    OME series multi-file pyramid support in tifffile.
            
                    Args:
                        record (logging.LogRecord): The log record being evaluated.
            
                    Returns:
                        bool: False if the log message matches the unwanted warning, True otherwise.
                    """
                    
                    if "OME series cannot read multi-file pyramids" in record.getMessage():
                        return False
                    
                    return True

            logger = tifffile.logger()
            logger.addFilter(IgnoreSpecificMessage())
            image_models_kwargs = dict(image_models_kwargs)
            assert (
                "c_coords" not in image_models_kwargs
            ), "The channel names for the morphology focus images are handled internally"
            image_models_kwargs["c_coords"] = list(channel_names.values())
            images["morphology_focus"] = _get_images(
                morphology_focus_dir,
                XeniumKeys.MORPHOLOGY_FOCUS_CHANNEL_IMAGE.format(0),
                imread_kwargs,
                image_models_kwargs)
            
            del image_models_kwargs["c_coords"]
            logger.removeFilter(IgnoreSpecificMessage())

    if table is not None:
        tables["table"] = table

    elements_dict = {"images": images, "labels": labels, "points": points, "tables": tables, "shapes": polygons}
    
    if cells_as_circles:
        elements_dict["shapes"][specs["region"]] = circles
    sdata = SpatialData(**elements_dict)

    # find and add additional aligned images
    if aligned_images:
        extra_images = _add_aligned_images(path, imread_kwargs, image_models_kwargs)
        
        for key, value in extra_images.items():
            sdata.images[key] = value

    return sdata


def _decode_cell_id_column(cell_id_column: pd.Series) -> pd.Series:
    """Decodes a column of cell IDs from bytes to UTF-8 strings, if necessary.

    This function checks if the first element of the input Series is a byte string.
    If so, it decodes all elements from bytes to UTF-8 strings. If the elements
    are already strings, it returns the Series unchanged.

    Args:
        cell_id_column (pd.Series): A pandas Series containing cell identifiers, 
            potentially encoded as byte strings.

    Returns:
        pd.Series: A Series of cell identifiers as UTF-8 decoded strings if input was in bytes;
            otherwise, returns the input Series unchanged.
    """
    
    if isinstance(cell_id_column.iloc[0], bytes):
        return cell_id_column.apply(lambda x: x.decode("utf-8"))
    
    return cell_id_column


def _get_polygons(
    path: Path, file: str, specs: dict[str, Any], n_jobs: int, 
    idx: ArrayLike | None = None) -> GeoDataFrame:
    """Load polygon boundaries from a file and return them as a GeoDataFrame.

    This function reads polygon boundary data from a file, constructs shapely
    Polygons from vertex coordinates, and returns a GeoDataFrame with optional
    validation and reindexing based on dataset version compatibility. It also
    applies a global transformation using the pixel size specified in the specs.
    
    Args:
        path (Path): Path to the dataset directory.
        file (str): Name of the file containing polygon boundaries.
        specs (dict[str, Any]): Dictionary containing metadata specifications
            (e.g., pixel size, dataset version).
        n_jobs (int): Number of parallel jobs to use when constructing polygons.
        idx (ArrayLike | None, optional): Optional array of indices to assign to
            the GeoDataFrame. Required for versions older than 2.0.0.
    
    Returns:
        GeoDataFrame: A GeoDataFrame containing the polygon geometries and
        associated metadata, transformed with appropriate scaling.
    
    Raises:
        AssertionError: If `idx` is required (for older dataset versions) and not
        provided or does not match expectations.
    
        UserWarning: If non-unique indices are found in newer dataset versions.
    
    Notes:
        - Polygons are constructed using all but the last vertex in each group,
          assuming a closed-loop representation.
        - For versions prior to 2.0.0, index matching with `idx` is enforced.
        - The pixel size from `specs` is used to apply a spatial transformation.
    """
    
    def _poly(arr: ArrayLike) -> Polygon:
        """Construct a Polygon from an array of coordinates.

        Creates a shapely Polygon using all but the last row of the input array,
        assuming the last point is a duplicate of the first for closure.
        
        Args:
            arr (ArrayLike): An array of shape (N, 2) representing a closed loop of
            (x, y) coordinates.
        
        Returns:
            Polygon: A shapely Polygon object created from the input coordinates.
        """
        
        return Polygon(arr[:-1])

    # seems to be faster than pd.read_parquet
    df = pq.read_table(path / file).to_pandas()

    group_by = df.groupby(XeniumKeys.CELL_ID)
    index = pd.Series(group_by.indices.keys())
    index = _decode_cell_id_column(index)
    out = Parallel(n_jobs=n_jobs)(
        delayed(_poly)(i.to_numpy())
        
        for _, i in group_by[[XeniumKeys.BOUNDARIES_VERTEX_X, XeniumKeys.BOUNDARIES_VERTEX_Y]])
    geo_df = GeoDataFrame({"geometry": out})
    version = _parse_version_of_xenium_analyzer(specs)
    
    if version is not None and version < packaging.version.parse("2.0.0"):
        assert idx is not None
        assert len(idx) == len(geo_df)
        assert np.unique(geo_df.index).size == len(geo_df)
        assert index.equals(idx)
        geo_df.index = idx
    else:
        geo_df.index = index
        
        if not np.unique(geo_df.index).size == len(geo_df):
            warnings.warn(
                "Found non-unique polygon indices, this will be addressed in a future version of the reader. For the "
                "time being please consider merging polygons with non-unique indices into single multi-polygons.",
                UserWarning,
                stacklevel=2,
            )
    scale = Scale([1.0 / specs["pixel_size"], 1.0 / specs["pixel_size"]], axes=("x", "y"))
    
    return ShapesModel.parse(geo_df, transformations={"global": scale})


def _get_labels_and_indices_mapping(
    path: Path, file: str, specs: dict[str, Any], mask_index: int, labels_name: str,
    labels_models_kwargs: Mapping[str, Any] = MappingProxyType({})) -> tuple[GeoDataFrame, pd.DataFrame | None]:
    """Extracts labeled regions and maps them to their corresponding cell IDs.
    
    This function reads a Zarr-based zip archive to retrieve labeled cell or nuclear masks,
    builds a labels model from the segmentation mask, and optionally maps label indices
    to cell IDs if supported by the version of the Xenium analyzer.
    
    Args:
        path (Path): Path to the directory containing the data.
        file (str): Name of the file used within the path (not used in this function).
        specs (dict[str, Any]): Dictionary containing metadata, including the Xenium analyzer version.
        mask_index (int): Index specifying which segmentation mask to use (0 for nuclei, 1 for cells).
        labels_name (str): Name of the region labels (e.g., "cells").
        labels_models_kwargs (Mapping[str, Any], optional): Additional keyword arguments passed to
            `Labels2DModel.parse`.
    
    Returns:
        tuple[GeoDataFrame, pd.DataFrame | None]: A tuple where the first element is a GeoDataFrame
        representing the labeled regions, and the second is a DataFrame mapping cell IDs to label indices,
        or None if mapping is not available for the analyzer version or mask index.
    
    Raises:
        ValueError: If `mask_index` is not 0 or 1, or if label indices do not match expected values
        in the data structure.
    """

    if mask_index not in [0, 1]:
        raise ValueError(f"mask_index must be 0 or 1, found {mask_index}.")

    with tempfile.TemporaryDirectory() as tmpdir:
        zip_file = path / XeniumKeys.CELLS_ZARR
        
        with zipfile.ZipFile(zip_file, "r") as zip_ref:
            zip_ref.extractall(tmpdir)

        with zarr.open(str(tmpdir), mode="r") as z:
            # get the labels
            masks = z["masks"][f"{mask_index}"][...]
            labels = Labels2DModel.parse(
                masks, dims=("y", "x"), transformations={"global": Identity()}, **labels_models_kwargs)

            # build the matching table
            version = _parse_version_of_xenium_analyzer(specs)
            
            if mask_index == 0:
                
                # nuclei currently not supported
                return labels, None
            
            if version is None or version is not None and version < packaging.version.parse("1.3.0"):
                
                # supported in version 1.3.0 and not supported in version 1.0.2; conservatively, let's assume it is not
                # supported in versions < 1.3.0
                return labels, None

            cell_id, dataset_suffix = z["cell_id"][...].T
            cell_id_str = cell_id_str_from_prefix_suffix_uint32(cell_id, dataset_suffix)

            # this information will probably be available in the `label_id` column for version > 2.0.0 (see public
            # release notes mentioned above)
            real_label_index = get_element_instances(labels).values

            # background removal
            if real_label_index[0] == 0:
                real_label_index = real_label_index[1:]

            if version < packaging.version.parse("2.0.0"):
                expected_label_index = z["seg_mask_value"][...]

                if not np.array_equal(expected_label_index, real_label_index):
                    raise ValueError(
                        "The label indices from the labels differ from the ones from the input data. Please report "
                        f"this issue. Real label indices: {real_label_index}, expected label indices: "
                        f"{expected_label_index}.")
            else:
                labels_positional_indices = z["polygon_sets"][mask_index]["cell_index"][...]
                
                if not np.array_equal(labels_positional_indices, np.arange(len(labels_positional_indices))):
                    raise ValueError(
                        "The positional indices of the labels do not match the expected range. Please report this "
                        "issue.")

            # labels_index is an uint32, so let's cast to np.int64 to avoid the risk of overflow on some systems
            indices_mapping = pd.DataFrame(
                {"region": labels_name,
                 "cell_id": cell_id_str,
                 "label_index": real_label_index.astype(np.int64)})
            
            return labels, indices_mapping


@inject_docs(xx=XeniumKeys)
def _get_cells_metadata_table_from_zarr(
    path: Path,
    file: str,
    specs: dict[str, Any]) -> AnnData:
    """Reads cell metadata from the `{xx.CELLS_ZARR}` archive.

    This function extracts and parses the `cell_summary` table from a Zarr zip archive.
    The summary includes different fields depending on the version of the Xenium analyzer:
    for versions < 2.0.0, it includes `z_level`, and for versions >= 2.0.0, it includes 
    `nucleus_count`.

    Args:
        path (Path): Path to the directory containing the `{xx.CELLS_ZARR}` file.
        file (str): Not used in the current implementation but kept for consistency.
        specs (dict[str, Any]): Dictionary containing metadata, including the Xenium analyzer version.

    Returns:
        AnnData: A DataFrame-like object containing cell metadata, including `cell_id` values.

    Notes:
        For versions >= 2.0.0, the function could be extended to parse the segmentation
        method used to generate the masks.
    """
    
    # for version >= 2.0.0, in this function we could also parse the segmentation method used to obtain the masks
    with tempfile.TemporaryDirectory() as tmpdir:
        zip_file = path / XeniumKeys.CELLS_ZARR
        
        with zipfile.ZipFile(zip_file, "r") as zip_ref:
            zip_ref.extractall(tmpdir)

        with zarr.open(str(tmpdir), mode="r") as z:
            x = z["cell_summary"][...]
            column_names = z["cell_summary"].attrs["column_names"]
            df = pd.DataFrame(x, columns=column_names)
            cell_id_prefix = z["cell_id"][:, 0]
            dataset_suffix = z["cell_id"][:, 1]
            cell_id_str = cell_id_str_from_prefix_suffix_uint32(cell_id_prefix, dataset_suffix)
            df[XeniumKeys.CELL_ID] = cell_id_str
            
            return df


def _get_points(path: Path, specs: dict[str, Any]) -> Table:
    """Reads transcript data from a parquet file and transforms the coordinates.

    This function reads transcript data from a parquet file located at `path`, decodes 
    the feature names from bytes (if applicable), and applies a coordinate transformation
    based on the provided specifications. It returns a `PointsModel` that contains the 
    transformed transcript points.
    
    Args:
        path (Path): The path to the directory containing the transcript parquet file.
        specs (dict[str, Any]): A dictionary containing metadata, including the pixel size 
                                 used for coordinate scaling.
    
    Returns:
        Table: A `PointsModel` containing the parsed and transformed transcript data, 
               including coordinates for `x`, `y`, and `z` positions, feature names, 
               and associated cell IDs.
    
    Notes:
        The coordinate transformation scales the `x` and `y` axes by the inverse of 
        the `pixel_size` specified in `specs`. This is applied globally to the data.
    """

    table = read_parquet(path / XeniumKeys.TRANSCRIPTS_FILE)
    table["feature_name"] = table["feature_name"].apply(
        lambda x: x.decode("utf-8") if isinstance(x, bytes) else str(x), meta=("feature_name", "object"))

    transform = Scale([1.0 / specs["pixel_size"], 1.0 / specs["pixel_size"]], axes=("x", "y"))
    points = PointsModel.parse(
        table,
        coordinates={"x": XeniumKeys.TRANSCRIPTS_X, "y": XeniumKeys.TRANSCRIPTS_Y, "z": XeniumKeys.TRANSCRIPTS_Z},
        feature_key=XeniumKeys.FEATURE_NAME,
        instance_key=XeniumKeys.CELL_ID,
        transformations={"global": transform},
        sort=True)
    
    return points


def _get_tables_and_circles(
    path: Path, cells_as_circles: bool, 
    specs: dict[str, Any]) -> AnnData | tuple[AnnData, AnnData]:
    """Reads and processes cell data, returning either a table or both a table and circles.

    This function reads the cell feature matrix and metadata files from the specified 
    `path`. It processes the data by extracting spatial coordinates and other metadata, 
    and adds them to an AnnData object. If `cells_as_circles` is `True`, it also calculates 
    the area of each cell and returns a `ShapesModel` representing the cells as circles 
    with corresponding radii.
    
    Args:
        path (Path): The path to the directory containing the cell feature matrix and metadata files.
        cells_as_circles (bool): Whether to represent cells as circles, with radii based on their area.
        specs (dict[str, Any]): A dictionary containing metadata, including the region and pixel size.
    
    Returns:
        AnnData | tuple[AnnData, AnnData]: 
            - If `cells_as_circles` is `False`, returns an `AnnData` object with processed metadata and spatial data.
            - If `cells_as_circles` is `True`, returns a tuple containing:
                - an `AnnData` object with processed metadata and spatial data (table),
                - a `ShapesModel` representing cells as circles with calculated radii.
    
    Notes:
        The function scales the spatial coordinates (`x` and `y`) based on the `pixel_size` provided 
        in the `specs` dictionary. The radii of the cells are calculated from the cell area, assuming 
        the cells are circular.
    
    Raises:
        AssertionError: If the `cell_id` in the metadata does not match the `obs_names` in the `adata` object.
    """

    adata = _read_10x_h5(path / XeniumKeys.CELL_FEATURE_MATRIX_FILE)
    metadata = pd.read_parquet(path / XeniumKeys.CELL_METADATA_FILE)
    np.testing.assert_array_equal(metadata.cell_id.astype(str), adata.obs_names.values)
    circ = metadata[[XeniumKeys.CELL_X, XeniumKeys.CELL_Y]].to_numpy()
    adata.obsm["spatial"] = circ
    metadata.drop([XeniumKeys.CELL_X, XeniumKeys.CELL_Y], axis=1, inplace=True)
    adata.obs = metadata
    adata.obs["region"] = specs["region"]
    adata.obs["region"] = adata.obs["region"].astype("category")
    adata.obs[XeniumKeys.CELL_ID] = _decode_cell_id_column(adata.obs[XeniumKeys.CELL_ID])
    table = TableModel.parse(adata, region=specs["region"], region_key="region", instance_key=str(XeniumKeys.CELL_ID))
    
    if cells_as_circles:
        transform = Scale([1.0 / specs["pixel_size"], 1.0 / specs["pixel_size"]], axes=("x", "y"))
        radii = np.sqrt(adata.obs[XeniumKeys.CELL_AREA].to_numpy() / np.pi)
        circles = ShapesModel.parse(
            circ,
            geometry=0,
            radius=radii,
            transformations={"global": transform},
            index=adata.obs[XeniumKeys.CELL_ID].copy())
        
        return table, circles
    
    return table


def _get_images(
    path: Path, file: str,
    imread_kwargs: Mapping[str, Any] = MappingProxyType({}),
    image_models_kwargs: Mapping[str, Any] = MappingProxyType({})) -> DataArray | DataTree:
    """Reads an image from a specified file, optionally adding a dummy channel, and returns it as a parsed model.

    This function loads an image from the file located at the specified `path` and `file`, using the provided 
    `imread_kwargs` for additional arguments to the `imread` function. If the `c_coords` key is provided in the 
    `image_models_kwargs` with a "dummy" value, it temporarily adds a dummy channel to the image to work around 
    a known issue with Napari's interpretation of 4-channel images. The image is then parsed into an `Image2DModel`.
    
    Args:
        path (Path): The directory containing the image file.
        file (str): The name of the image file to be read.
        imread_kwargs (Mapping[str, Any], optional): A mapping of additional keyword arguments to pass to the 
            `imread` function. Defaults to an empty mapping.
        image_models_kwargs (Mapping[str, Any], optional): A mapping of additional keyword arguments to pass 
            to the `Image2DModel.parse` method. Defaults to an empty mapping.
    
    Returns:
        DataArray | DataTree: The image parsed into an `Image2DModel`, which may be returned as a `DataArray` or 
        `DataTree` depending on the input data.
    
    Notes:
        If the `c_coords` key in `image_models_kwargs` contains the value "dummy", a dummy channel is added to 
        the image. This is a temporary workaround for an issue with Napari, where 4-channel images are currently 
        interpreted as RGB. This issue will be addressed in upcoming pull requests but has not been merged yet.
    
    Raises:
        FileNotFoundError: If the image file does not exist at the specified path.
    """
    
    image = imread(path / file, **imread_kwargs)
    
    if "c_coords" in image_models_kwargs and "dummy" in image_models_kwargs["c_coords"]:
        # Napari currently interprets 4 channel images as RGB; a series of PRs to fix this is almost ready but they will
        # not be merged soon.
        # Here, since the new data from the xenium analyzer version 2.0.0 gives 4-channel images that are not RGBA,
        # let's add a dummy channel as a temporary workaround.
        image = da.concatenate([image, da.zeros_like(image[0:1])], axis=0)
        
    return Image2DModel.parse(image, transformations={"global": Identity()}, 
                              dims=("c", "y", "x"), rgb=None, **image_models_kwargs)


def _add_aligned_images(
    path: Path,
    imread_kwargs: Mapping[str, Any] = MappingProxyType({}),
    image_models_kwargs: Mapping[str, Any] = MappingProxyType({})) -> dict[str, DataTree]:
    """Discover and parse aligned images in the specified directory.

    This function searches the specified `path` for `.ome.tif` and `.csv` files, identifies aligned image files 
    based on their suffix, and attempts to find corresponding alignment files. If alignment files are found, 
    it parses the aligned images using the `xenium_aligned_image` function and stores them in a dictionary. 
    The keys of the dictionary are the element names derived from the file suffixes.

    Args:
        path (Path): The directory containing the image and alignment files.
        imread_kwargs (Mapping[str, Any], optional): A mapping of additional keyword arguments to pass to the 
            `imread` function when reading the image. Defaults to an empty mapping.
        image_models_kwargs (Mapping[str, Any], optional): A mapping of additional keyword arguments to pass 
            to the image parsing function (`xenium_aligned_image`). Defaults to an empty mapping.

    Returns:
        dict[str, DataTree]: A dictionary where the keys are element names (derived from the suffix of the 
        aligned image files), and the values are the parsed images as `DataTree` objects.

    Notes:
        The function assumes that the aligned image files have specific suffixes (`XeniumKeys.ALIGNED_HE_IMAGE_SUFFIX` 
        and `XeniumKeys.ALIGNED_IF_IMAGE_SUFFIX`) and that corresponding alignment files (with a `.csv` extension) 
        should be present in the same directory. If more than one alignment file is found for an image, an assertion 
        error is raised.
        
        The images are parsed using the `xenium_aligned_image` function, and any additional keyword arguments for 
        reading or parsing can be provided through `imread_kwargs` and `image_models_kwargs`.

    Raises:
        AssertionError: If more than one alignment file is found for a given image.
    """

    images = {}
    ome_tif_files = list(path.glob("*.ome.tif"))
    csv_files = list(path.glob("*.csv"))
    
    for file in ome_tif_files:
        element_name = None
        
        for suffix in [XeniumKeys.ALIGNED_HE_IMAGE_SUFFIX, XeniumKeys.ALIGNED_IF_IMAGE_SUFFIX]:
            
            if file.name.endswith(suffix):
                element_name = suffix.replace(XeniumKeys.ALIGNMENT_FILE_SUFFIX_TO_REMOVE, "")
                break
            
        if element_name is not None:
            # check if an alignment file exists
            expected_filename = file.name.replace(
                XeniumKeys.ALIGNMENT_FILE_SUFFIX_TO_REMOVE, XeniumKeys.ALIGNMENT_FILE_SUFFIX_TO_ADD)
            alignment_files = [f for f in csv_files if f.name == expected_filename]
            assert len(alignment_files) <= 1, f"Found more than one alignment file for {file.name}."
            alignment_file = alignment_files[0] if alignment_files else None

            # parse the image
            image = xenium_aligned_image(file, alignment_file, imread_kwargs, image_models_kwargs)
            images[element_name] = image
            
    return images


def xenium_aligned_image(
    image_path: str | Path,
    alignment_file: str | Path | None,
    imread_kwargs: Mapping[str, Any] = MappingProxyType({}),
    image_models_kwargs: Mapping[str, Any] = MappingProxyType({}),
    dims: tuple[str, ...] | None = None,
    rgba: bool = False,
    c_coords: list[str] | None = None) -> DataTree:
    
    """
    Read an image aligned to a Xenium dataset, with an optional alignment file.

    This function reads an image and, if provided, applies an alignment transformation from the alignment file.
    It also handles specific image formats and shapes, interpreting the image dimensions and channels as needed.
    If the alignment file is not provided, it is assumed that the image is already aligned.

    Args:
        image_path (str | Path): Path to the image file.
        alignment_file (str | Path | None): Path to the alignment file. If not provided, it is assumed the image is aligned.
        imread_kwargs (Mapping[str, Any], optional): Keyword arguments to pass to the `imread` function. Defaults to an empty mapping.
        image_models_kwargs (Mapping[str, Any], optional): Additional keyword arguments to pass to the image parsing function. Defaults to an empty mapping.
        dims (tuple[str, ...] | None, optional): A tuple specifying the dimensions of the image (axes names). If not provided, the function attempts to infer the dimensions based on the image shape. Default is `None`.
        rgba (bool, optional): If `True`, interprets the `c` channel as RGBA, setting the channel names to `r`, `g`, `b`, and `a`. Defaults to `False`.
        c_coords (list[str] | None, optional): A list of channel names for the image. If not provided, the function tries to infer them based on the image shape and name. Default is `None`.

    Returns:
        DataTree: A parsed image model, either single-scale or multi-scale, aligned according to the provided transformation.

    Raises:
        AssertionError: If the image or alignment file does not exist at the given path.
        ValueError: If the image or alignment file has an invalid shape or dimension.

    Notes:
        - The image is parsed based on its shape and provided dimensions. If `dims` is `None`, the function attempts to automatically infer the image layout.
        - If `alignment_file` is provided, the function will read and apply the alignment transformation using the file.
        - If the `rgba` argument is `True` or the image is detected as an H&E image, the channel names will be set to `r`, `g`, `b`, and `a`.
        - The function expects the image to be a 2D or 3D array, where the channels (`c`), height (`y`), and width (`x`) are properly aligned according to the provided or inferred dimensions.
    """

    image_path = Path(image_path)
    assert image_path.exists(), f"File {image_path} does not exist."
    image = imread(image_path, **imread_kwargs)

    # Depending on the version of pipeline that was used, some images have shape (1, y, x, 3) and others (3, y, x) or
    # (4, y, x).
    # since y and x are always different from 1, let's differentiate from the two cases here, independently of the
    # pipeline version.
    # Note that a more robust approach is to look at the xml metadata in the ome.tif; we should use this in a future PR.
    # In fact, it could be that the len(image.shape) == 4 has actually dimes (1, x, y, c) and not (1, y, x, c). This is
    # not a problem because the transformation is constructed to be consistent, but if is the case, the data orientation
    # would be transposed compared to the original image, not ideal.
    if dims is None:
        
        if len(image.shape) == 4:
            assert image.shape[0] == 1
            assert image.shape[-1] == 3
            image = image.squeeze(0)
            dims = ("y", "x", "c")
        else:
            assert len(image.shape) == 3
            assert image.shape[0] in [3, 4]
            dims = ("c", "y", "x")
    else:
        logging.info(f"Image has shape {image.shape}, parsing with dims={dims}.")
        image = DataArray(image, dims=dims)
        # squeeze spurious dimensions away
        to_squeeze = [dim for dim in dims if dim not in ["c", "x", "y"]]
        dims = tuple(dim for dim in dims if dim in ["c", "x", "y"])
        
        for dim in to_squeeze:
            image = image.squeeze(dim)

    if alignment_file is None:
        transformation = Identity()
    else:
        alignment_file = Path(alignment_file)
        assert alignment_file.exists(), f"File {alignment_file} does not exist."
        alignment = pd.read_csv(alignment_file, header=None).values
        transformation = Affine(alignment, input_axes=("x", "y"), output_axes=("x", "y"))

    if c_coords is None and (rgba or image_path.name.endswith(XeniumKeys.ALIGNED_HE_IMAGE_SUFFIX)):
        c_index = dims.index("c")
        n_channels = image.shape[c_index]
        
        if n_channels == 3:
            c_coords = ["r", "g", "b"]
        elif n_channels == 4:
            c_coords = ["r", "g", "b", "a"]

    return Image2DModel.parse(
        image,
        dims=dims,
        transformations={"global": transformation},
        c_coords=c_coords,
        **image_models_kwargs)


def _selection_to_polygon(df: pd.DataFrame, pixel_size: float) -> Polygon:
    """
    Convert a DataFrame of selected points into a polygon, scaled by the given pixel size.
    
    This function takes a DataFrame containing the X and Y coordinates of selected points and converts them into a 
    polygon object. The coordinates are scaled by dividing by the provided pixel size to transform the points to
    the appropriate scale.
    
    Args:
        df (pd.DataFrame): DataFrame containing the X and Y coordinates of the selected points. The DataFrame must 
                            include columns for the X and Y coordinates as specified by `XeniumKeys.EXPLORER_SELECTION_X`
                            and `XeniumKeys.EXPLORER_SELECTION_Y`.
        pixel_size (float): The pixel size used to scale the X and Y coordinates.
    
    Returns:
        Polygon: A polygon object representing the selected points after scaling by the pixel size.
    
    Notes:
        - The function expects the DataFrame `df` to contain columns for the X and Y coordinates as specified by 
          `XeniumKeys.EXPLORER_SELECTION_X` and `XeniumKeys.EXPLORER_SELECTION_Y`.
        - The coordinates are scaled by dividing by the provided `pixel_size` to adjust for the scale.
    """

    xy_keys = [XeniumKeys.EXPLORER_SELECTION_X, XeniumKeys.EXPLORER_SELECTION_Y]
    
    return Polygon(df[xy_keys].values / pixel_size)


def xenium_explorer_selection(
    path: str | Path, pixel_size: float = 0.2125, 
    return_list: bool = False) -> Polygon | list[Polygon]:
    """
    Read coordinates from a Xenium Explorer selection CSV file and return a Polygon or a list of Polygons.

    This function processes a `.csv` file exported from the Xenium Explorer, which can be generated by either the 
    "Freehand Selection" or "Rectangular Selection" tools. The output Polygon can be used for polygon queries in the 
    pixel coordinate system, with the default being the global coordinate system for Xenium data. If the file was 
    generated using `spatialdata_xenium_explorer`, the `pixel_size` parameter should match the one used during the 
    conversion.

    If multiple polygons were selected and exported to a single file, a list of Polygons will be returned.

    Args:
        path (str | Path): The path to the `.csv` file containing the selection coordinates exported from the Xenium Explorer.
        pixel_size (float, optional): The size of a pixel in microns. The default value is `0.2125`, which is the 
                                      standard Xenium pixel size. If the file was converted using `spatialdata_xenium_explorer`,
                                      this value must match the pixel size used during conversion.
        return_list (bool, optional): If `True`, the function will return a list of Polygons, even if only one polygon
                                      was selected. Defaults to `False`.

    Returns:
        Polygon | list[Polygon]: The function returns a `Polygon` if only one polygon was selected. If multiple polygons
                                  were selected, it returns a list of `Polygon` objects.
    
    Raises:
        ValueError: If the CSV file does not contain the expected selection data, an exception may be raised.

    Notes:
        - The function assumes the `.csv` file is properly formatted, with the necessary coordinate columns.
        - If the `.csv` contains multiple selections, the function will return a list of polygons corresponding to each selection.
    """
    
    df = pd.read_csv(path, skiprows=2)

    if XeniumKeys.EXPLORER_SELECTION_KEY not in df:
        polygon = _selection_to_polygon(df, pixel_size)
        
        return [polygon] if return_list else polygon

    return [_selection_to_polygon(sub_df, pixel_size) for _, sub_df in df.groupby(XeniumKeys.EXPLORER_SELECTION_KEY)]


def _parse_version_of_xenium_analyzer(
        specs: dict[str, Any], 
        hide_warning: bool = True) -> packaging.version.Version | None:
    """
    Parse the version of the Xenium Analyzer from the provided specs dictionary.
    
    This function extracts the version information from the `specs` dictionary, specifically from either the 
    `xenium_ranger` key (if present) or the `analysis_sw_version` key. The version string is parsed using a regular 
    expression to identify the version of the Xenium Analyzer software. If the version cannot be parsed, a warning is 
    issued (unless suppressed), and `None` is returned.
    
    Args:
        specs (dict[str, Any]): A dictionary containing the metadata or specifications, which includes either the
                                 version information from the `xenium_ranger` or `analysis_sw_version`.
        hide_warning (bool, optional): If `True`, suppresses the warning when the version string cannot be parsed. 
                                       Defaults to `True`.
    
    Returns:
        packaging.version.Version | None: Returns the parsed version as a `packaging.version.Version` object if 
                                           the version is successfully parsed. Returns `None` if parsing fails.
    
    Notes:
        - The function attempts to parse the version in the format `xenium-<major>.<minor>.<patch>.<build>` or `xenium-<major>.<minor>.<patch>-<additional>`.
        - If the version cannot be parsed, a warning is issued, and `None` is returned (unless suppressed by the `hide_warning` parameter).
    
    Example:
        If the `specs` dictionary contains:
            {"xenium_ranger": {"version": "xenium-2.0.0.6-35-ga7e17149a"}}
        The function will return:
            packaging.version.parse("2.0.0.6-35")
    """

    # After using xeniumranger (e.g. 3.0.1.1) to resegment data from previous versions (e.g. xenium-1.6.0.7), a new dict is added to
    # `specs`, named 'xenium_ranger', which contains the key 'version' and whose value specifies the version of xeniumranger used to
    # resegment the data (e.g. 'xenium-3.0.1.1').
    # When parsing the outs/ folder from the resegmented data, this version (rather than the original 'analysis_sw_version') is used
    # whenever a code branch is dependent on the data version
    if specs.get(XeniumKeys.XENIUM_RANGER):
        string = specs[XeniumKeys.XENIUM_RANGER]["version"]
    else:
        string = specs[XeniumKeys.ANALYSIS_SW_VERSION]

    pattern = r"^(?:x|X)enium-(\d+\.\d+\.\d+(\.\d+-\d+)?)"
    result = re.search(pattern, string)
    warning_message = (
        f"Could not parse the version of the Xenium Analyzer from the string: {string}. This may happen for "
        "experimental version of the data. Please report in GitHub https://github.com/scverse/spatialdata-io/issues.\n"
        "The reader will continue assuming the latest version of the Xenium Analyzer.")

    if result is None:
        
        if not hide_warning:
            warnings.warn(warning_message, stacklevel=2)
            
        return None

    group = result.groups()[0]
    
    try:
        return packaging.version.parse(group)
    except packaging.version.InvalidVersion:
        
        if not hide_warning:
            warnings.warn(warning_message, stacklevel=2)
            
        return None


def cell_id_str_from_prefix_suffix_uint32(
        cell_id_prefix: ArrayLike, 
        dataset_suffix: ArrayLike) -> ArrayLike:
    """
    Convert cell ID prefix and dataset suffix into a standardized string format.
    
    This function takes a pair of arrays: the `cell_id_prefix` (an array of unsigned 32-bit integers) and the 
    `dataset_suffix` (an array of dataset suffixes), and constructs a standardized cell ID string for each pair. 
    The `cell_id_prefix` is first converted to hexadecimal, then shifted according to a predefined mapping, and 
    finally, it is concatenated with the corresponding `dataset_suffix` to form the complete cell ID string.
    
    Args:
        cell_id_prefix (ArrayLike): An array of unsigned 32-bit integers representing the prefix of the cell IDs.
        dataset_suffix (ArrayLike): An array of dataset suffixes corresponding to the cell IDs.
    
    Returns:
        ArrayLike: An array of strings representing the full cell IDs, where each cell ID is formatted as 
                   "<prefix>-<suffix>", with the prefix being a shifted hexadecimal string.
    
    Notes:
        - The `cell_id_prefix` is converted to hexadecimal, with the `0x` prefix removed.
        - The hexadecimal digits are then shifted based on a predefined mapping (e.g., '0' becomes 'a', '1' becomes 'b', etc.).
        - The result is a string of the form "<shifted_hex_prefix>-<dataset_suffix>".
    
    Example:
        If `cell_id_prefix` is `[12345, 67890]` and `dataset_suffix` is `['001', '002']`, the function will return:
        `["aaaa-001", "bbbb-002"]`.
    
    See also:
        The explanation of the cell ID format can be found here: 
        https://www.10xgenomics.com/support/software/xenium-onboard-analysis/latest/analysis/xoa-output-zarr#cellID
    """
    
    cell_id_prefix_hex = [hex(x)[2:] for x in cell_id_prefix]

    # shift the hex values
    hex_shift = {str(i): chr(ord("a") + i) for i in range(10)} | {
        chr(ord("a") + i): chr(ord("a") + 10 + i) for i in range(6)}
    cell_id_prefix_hex_shifted = ["".join([hex_shift[c] for c in x]) for x in cell_id_prefix_hex]

    # merge the prefix and the suffix
    cell_id_str = [str(x[0]).rjust(8, "a") + f"-{x[1]}" for x in zip(cell_id_prefix_hex_shifted, dataset_suffix)]

    return np.array(cell_id_str)


def prefix_suffix_uint32_from_cell_id_str(cell_id_str: ArrayLike) -> tuple[ArrayLike, ArrayLike]:
    """
    Convert cell ID strings into their corresponding prefix and dataset suffix.
    
    This function takes an array of cell ID strings in the format "<prefix>-<suffix>", where the prefix is a 
    shifted hexadecimal string and the suffix is a numerical identifier. It splits the cell ID string into 
    the prefix and suffix, unshifts the hexadecimal prefix back to its original form, and then converts the 
    prefix to an unsigned 32-bit integer. The result is a tuple containing two arrays: the prefix as a 
    32-bit unsigned integer array and the dataset suffix as an integer array.
    
    Args:
        cell_id_str (ArrayLike): An array of cell ID strings, each in the format "<shifted_hex_prefix>-<suffix>".
    
    Returns:
        tuple[ArrayLike, ArrayLike]: A tuple containing:
            - An array of unsigned 32-bit integers representing the unshifted hexadecimal cell ID prefixes.
            - An array of integers representing the dataset suffixes.
    
    Example:
        If `cell_id_str` is `["aaaa-001", "bbbb-002"]`, the function will return:
        (array([12345, 67890], dtype=np.uint32), array([1, 2])).
    
    Notes:
        - The prefix is first reversed from its shifted hexadecimal form back to the original hexadecimal form.
        - The hexadecimal prefix is then converted into a 32-bit unsigned integer.
        - The suffix is simply converted to an integer.
    
    See also:
        The explanation of the cell ID format can be found here: 
        https://www.10xgenomics.com/support/software/xenium-onboard-analysis/latest/analysis/xoa-output-zarr#cellID
    """
    
    # parse the string into the prefix and suffix
    cell_id_prefix_str, dataset_suffix = zip(*[x.split("-") for x in cell_id_str])
    dataset_suffix_int = [int(x) for x in dataset_suffix]

    # reverse the shifted hex conversion
    hex_unshift = {chr(ord("a") + i): str(i) for i in range(10)} | {
        chr(ord("a") + 10 + i): chr(ord("a") + i) for i in range(6)}
    cell_id_prefix_hex = ["".join([hex_unshift[c] for c in x]) for x in cell_id_prefix_str]

    # Convert hex (no need to add the 0x prefix)
    cell_id_prefix = [int(x, 16) for x in cell_id_prefix_hex]

    return np.array(cell_id_prefix, dtype=np.uint32), np.array(dataset_suffix_int)
